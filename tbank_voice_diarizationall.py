#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
T-BANK VOICEKIT –° –ê–ù–ê–õ–ò–ó–û–ú –ì–û–õ–û–°–û–í (–°–¢–ï–†–ï–û –í–ï–†–°–ò–Ø)
–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è T-Bank VoiceKit —Å –∞–Ω–∞–ª–∏–∑–æ–º –≥–æ–ª–æ—Å–æ–≤ –¥–ª—è —Å—Ç–µ—Ä–µ–æ –∞—É–¥–∏–æ
"""

from tinkoff_voicekit_client import ClientSTT
from pydub import AudioSegment
import os
import json
import numpy as np
import librosa
from typing import Dict, List, Optional, Tuple
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def extract_voice_features(audio_segment: AudioSegment, start_time: float, end_time: float) -> Dict:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –≥–æ–ª–æ—Å–∞ –∏–∑ –∞—É–¥–∏–æ —Å–µ–≥–º–µ–Ω—Ç–∞ (–ø–æ–¥–¥–µ—Ä–∂–∫–∞ —Å—Ç–µ—Ä–µ–æ)
    """
    try:
        # –ü–æ–ª—É—á–∞–µ–º –∫–∞–Ω–∞–ª—ã –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è —Å—Ç–µ—Ä–µ–æ –∞—É–¥–∏–æ
        if audio_segment.channels == 2:
            # –†–∞–∑–¥–µ–ª—è–µ–º —Å—Ç–µ—Ä–µ–æ –Ω–∞ –ª–µ–≤—ã–π –∏ –ø—Ä–∞–≤—ã–π –∫–∞–Ω–∞–ª—ã
            left_channel = audio_segment.split_to_mono()[0]
            right_channel = audio_segment.split_to_mono()[1]
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–µ–≤—ã–π –∫–∞–Ω–∞–ª –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–π
            samples = np.array(left_channel.get_array_of_samples(), dtype=np.float32)
            sample_rate = left_channel.frame_rate
            
            # –¢–∞–∫–∂–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–∞–≤—ã–π –∫–∞–Ω–∞–ª –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            right_samples = np.array(right_channel.get_array_of_samples(), dtype=np.float32)
        else:
            # –ú–æ–Ω–æ –∞—É–¥–∏–æ
            samples = np.array(audio_segment.get_array_of_samples(), dtype=np.float32)
            sample_rate = audio_segment.frame_rate
            right_samples = None
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
        if len(samples) > 0:
            samples = samples / np.max(np.abs(samples))
        
        if right_samples is not None and len(right_samples) > 0:
            right_samples = right_samples / np.max(np.abs(right_samples))
        
        # –í—ã—á–∏—Å–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã
        start_sample = int(start_time * sample_rate)
        end_sample = int(end_time * sample_rate)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–µ–≥–º–µ–Ω—Ç
        segment_samples = samples[start_sample:end_sample]
        right_segment_samples = right_samples[start_sample:end_sample] if right_samples is not None else None
        
        if len(segment_samples) == 0:
            return {
                "fundamental_frequency": 0,
                "spectral_centroid": 0,
                "spectral_rolloff": 0,
                "zero_crossing_rate": 0,
                "mfcc": [0] * 13,
                "energy": 0,
                "stereo_balance": 0,
                "channel_difference": 0
            }
        
        # –û—Å–Ω–æ–≤–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ (F0) –¥–ª—è –ª–µ–≤–æ–≥–æ –∫–∞–Ω–∞–ª–∞
        f0_left = librosa.yin(segment_samples, fmin=50, fmax=400, sr=sample_rate)
        fundamental_freq_left = np.median(f0_left[f0_left > 0]) if len(f0_left[f0_left > 0]) > 0 else 0
        
        # –û—Å–Ω–æ–≤–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ –¥–ª—è –ø—Ä–∞–≤–æ–≥–æ –∫–∞–Ω–∞–ª–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
        fundamental_freq_right = 0
        if right_segment_samples is not None and len(right_segment_samples) > 0:
            f0_right = librosa.yin(right_segment_samples, fmin=50, fmax=400, sr=sample_rate)
            fundamental_freq_right = np.median(f0_right[f0_right > 0]) if len(f0_right[f0_right > 0]) > 0 else 0
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∏–ª–∏ –ª–µ–≤—ã–π –∫–∞–Ω–∞–ª
        fundamental_freq = (fundamental_freq_left + fundamental_freq_right) / 2 if fundamental_freq_right > 0 else fundamental_freq_left
        
        # –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –¥–ª—è –ª–µ–≤–æ–≥–æ –∫–∞–Ω–∞–ª–∞
        spectral_centroid_left = librosa.feature.spectral_centroid(y=segment_samples, sr=sample_rate)[0]
        spectral_rolloff_left = librosa.feature.spectral_rolloff(y=segment_samples, sr=sample_rate)[0]
        zero_crossing_rate_left = librosa.feature.zero_crossing_rate(segment_samples)[0]
        
        # –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –¥–ª—è –ø—Ä–∞–≤–æ–≥–æ –∫–∞–Ω–∞–ª–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
        spectral_centroid_right = np.array([0])
        spectral_rolloff_right = np.array([0])
        zero_crossing_rate_right = np.array([0])
        
        if right_segment_samples is not None and len(right_segment_samples) > 0:
            spectral_centroid_right = librosa.feature.spectral_centroid(y=right_segment_samples, sr=sample_rate)[0]
            spectral_rolloff_right = librosa.feature.spectral_rolloff(y=right_segment_samples, sr=sample_rate)[0]
            zero_crossing_rate_right = librosa.feature.zero_crossing_rate(right_segment_samples)[0]
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
        spectral_centroid = (np.mean(spectral_centroid_left) + np.mean(spectral_centroid_right)) / 2
        spectral_rolloff = (np.mean(spectral_rolloff_left) + np.mean(spectral_rolloff_right)) / 2
        zero_crossing_rate = (np.mean(zero_crossing_rate_left) + np.mean(zero_crossing_rate_right)) / 2
        
        # MFCC –¥–ª—è –ª–µ–≤–æ–≥–æ –∫–∞–Ω–∞–ª–∞
        mfcc_left = librosa.feature.mfcc(y=segment_samples, sr=sample_rate, n_mfcc=13)
        
        # MFCC –¥–ª—è –ø—Ä–∞–≤–æ–≥–æ –∫–∞–Ω–∞–ª–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
        mfcc_right = np.zeros_like(mfcc_left)
        if right_segment_samples is not None and len(right_segment_samples) > 0:
            mfcc_right = librosa.feature.mfcc(y=right_segment_samples, sr=sample_rate, n_mfcc=13)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è MFCC
        mfcc = (np.mean(mfcc_left, axis=1) + np.mean(mfcc_right, axis=1)) / 2
        
        # –≠–Ω–µ—Ä–≥–∏—è –¥–ª—è –æ–±–æ–∏—Ö –∫–∞–Ω–∞–ª–æ–≤
        energy_left = np.sum(segment_samples ** 2)
        energy_right = np.sum(right_segment_samples ** 2) if right_segment_samples is not None else 0
        energy = energy_left + energy_right
        
        # –°—Ç–µ—Ä–µ–æ –±–∞–ª–∞–Ω—Å (—Ä–∞–∑–Ω–æ—Å—Ç—å –º–µ–∂–¥—É –∫–∞–Ω–∞–ª–∞–º–∏)
        stereo_balance = (energy_left - energy_right) / (energy_left + energy_right + 1e-10)
        
        # –†–∞–∑–Ω–æ—Å—Ç—å –º–µ–∂–¥—É –∫–∞–Ω–∞–ª–∞–º–∏ –ø–æ –æ—Å–Ω–æ–≤–Ω–æ–π —á–∞—Å—Ç–æ—Ç–µ
        channel_difference = abs(fundamental_freq_left - fundamental_freq_right) if fundamental_freq_right > 0 else 0
        
        return {
            "fundamental_frequency": float(fundamental_freq),
            "spectral_centroid": float(spectral_centroid),
            "spectral_rolloff": float(spectral_rolloff),
            "zero_crossing_rate": float(zero_crossing_rate),
            "mfcc": [float(x) for x in mfcc],
            "energy": float(energy),
            "stereo_balance": float(stereo_balance),
            "channel_difference": float(channel_difference),
            "left_f0": float(fundamental_freq_left),
            "right_f0": float(fundamental_freq_right)
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –≥–æ–ª–æ—Å–∞: {e}")
        return {
            "fundamental_frequency": 0,
            "spectral_centroid": 0,
            "spectral_rolloff": 0,
            "zero_crossing_rate": 0,
            "mfcc": [0] * 13,
            "energy": 0,
            "stereo_balance": 0,
            "channel_difference": 0,
            "left_f0": 0,
            "right_f0": 0
        }

def analyze_speaker_voice_profile(voice_features: List[Dict]) -> Dict:
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ—Ñ–∏–ª—å –≥–æ–ª–æ—Å–∞ —Å–ø–∏–∫–µ—Ä–∞ (–ø–æ–¥–¥–µ—Ä–∂–∫–∞ —Å—Ç–µ—Ä–µ–æ)
    """
    if not voice_features:
        return {"speaker_id": "UNKNOWN", "confidence": 0}
    
    # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
    avg_f0 = np.mean([f["fundamental_frequency"] for f in voice_features])
    avg_spectral_centroid = np.mean([f["spectral_centroid"] for f in voice_features])
    avg_spectral_rolloff = np.mean([f["spectral_rolloff"] for f in voice_features])
    avg_zcr = np.mean([f["zero_crossing_rate"] for f in voice_features])
    avg_energy = np.mean([f["energy"] for f in voice_features])
    
    # –°—Ç–µ—Ä–µ–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
    avg_stereo_balance = np.mean([f["stereo_balance"] for f in voice_features])
    avg_channel_difference = np.mean([f["channel_difference"] for f in voice_features])
    avg_left_f0 = np.mean([f["left_f0"] for f in voice_features])
    avg_right_f0 = np.mean([f["right_f0"] for f in voice_features])
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –≥–æ–ª–æ—Å–∞ –ø–æ –æ—Å–Ω–æ–≤–Ω–æ–π —á–∞—Å—Ç–æ—Ç–µ
    if avg_f0 < 120:
        voice_type = "MALE_LOW"
    elif avg_f0 < 180:
        voice_type = "MALE_MID"
    elif avg_f0 < 250:
        voice_type = "FEMALE_LOW"
    else:
        voice_type = "FEMALE_HIGH"
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–∑–∏—Ü–∏—é —Å–ø–∏–∫–µ—Ä–∞ –≤ —Å—Ç–µ—Ä–µ–æ
    if abs(avg_stereo_balance) < 0.1:
        stereo_position = "CENTER"
    elif avg_stereo_balance > 0.1:
        stereo_position = "LEFT"
    else:
        stereo_position = "RIGHT"
    
    # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –≥–æ–ª–æ—Å–∞
    f0_variance = np.var([f["fundamental_frequency"] for f in voice_features])
    stability = 1.0 / (1.0 + f0_variance / 1000)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
    
    # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å —Å—Ç–µ—Ä–µ–æ –ø–æ–∑–∏—Ü–∏–∏
    stereo_variance = np.var([f["stereo_balance"] for f in voice_features])
    stereo_stability = 1.0 / (1.0 + stereo_variance / 0.1)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
    
    return {
        "voice_type": voice_type,
        "stereo_position": stereo_position,
        "avg_fundamental_frequency": avg_f0,
        "avg_spectral_centroid": avg_spectral_centroid,
        "avg_spectral_rolloff": avg_spectral_rolloff,
        "avg_zero_crossing_rate": avg_zcr,
        "avg_energy": avg_energy,
        "avg_stereo_balance": avg_stereo_balance,
        "avg_channel_difference": avg_channel_difference,
        "avg_left_f0": avg_left_f0,
        "avg_right_f0": avg_right_f0,
        "stability": stability,
        "stereo_stability": stereo_stability,
        "sample_count": len(voice_features)
    }

def identify_speaker_by_voice(voice_features: Dict, known_speakers: List[Dict]) -> Tuple[str, float]:
    """
    –ò–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Å–ø–∏–∫–µ—Ä–∞ –ø–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º –≥–æ–ª–æ—Å–∞ (–ø–æ–¥–¥–µ—Ä–∂–∫–∞ —Å—Ç–µ—Ä–µ–æ)
    """
    if not known_speakers:
        return "SPEAKER_01", 0.5
    
    best_match = None
    best_score = 0
    
    for speaker in known_speakers:
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å –ø–æ –æ—Å–Ω–æ–≤–Ω–æ–π —á–∞—Å—Ç–æ—Ç–µ
        avg_f0 = speaker.get("avg_fundamental_frequency", 0)
        if avg_f0 == 0:
            avg_f0 = np.mean([f["fundamental_frequency"] for f in speaker.get("voice_features", [])]) if speaker.get("voice_features") else 0
        
        f0_diff = abs(voice_features["fundamental_frequency"] - avg_f0)
        f0_score = 1.0 / (1.0 + f0_diff / 50)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å –ø–æ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º
        avg_spectral = speaker.get("avg_spectral_centroid", 0)
        if avg_spectral == 0:
            avg_spectral = np.mean([f["spectral_centroid"] for f in speaker.get("voice_features", [])]) if speaker.get("voice_features") else 0
        
        spectral_diff = abs(voice_features["spectral_centroid"] - avg_spectral)
        spectral_score = 1.0 / (1.0 + spectral_diff / 1000)
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å –ø–æ MFCC
        mfcc_diff = np.linalg.norm(
            np.array(voice_features["mfcc"]) - np.array(speaker.get("avg_mfcc", [0] * 13))
        )
        mfcc_score = 1.0 / (1.0 + mfcc_diff / 10)
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å –ø–æ —Å—Ç–µ—Ä–µ–æ –ø–æ–∑–∏—Ü–∏–∏
        avg_stereo = speaker.get("avg_stereo_balance", 0)
        stereo_diff = abs(voice_features["stereo_balance"] - avg_stereo)
        stereo_score = 1.0 / (1.0 + stereo_diff / 0.2)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å –ø–æ —Ä–∞–∑–Ω–æ—Å—Ç–∏ –∫–∞–Ω–∞–ª–æ–≤
        avg_channel = speaker.get("avg_channel_difference", 0)
        channel_diff = abs(voice_features["channel_difference"] - avg_channel)
        channel_score = 1.0 / (1.0 + channel_diff / 20)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
        
        # –û–±—â–∏–π score —Å —É—á–µ—Ç–æ–º —Å—Ç–µ—Ä–µ–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
        total_score = (f0_score * 0.3 + spectral_score * 0.2 + mfcc_score * 0.2 + 
                      stereo_score * 0.2 + channel_score * 0.1)
        
        if total_score > best_score:
            best_score = total_score
            best_match = speaker["speaker_id"]
    
    # –ï—Å–ª–∏ score —Å–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∏–π, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞
    if best_score < 0.3:  # –ù–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è –ª—É—á—à–µ–π –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏
        new_speaker_id = f"SPEAKER_{len(known_speakers) + 1:02d}"
        return new_speaker_id, 0.25
    
    return best_match, best_score

def tbank_with_voice_analysis_diarization(transcript_data: Dict, audio_file: str) -> Dict:
    """
    –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º T-Bank VoiceKit –∏ –∞–Ω–∞–ª–∏–∑–∞ –≥–æ–ª–æ—Å–æ–≤ (–ø–æ–¥–¥–µ—Ä–∂–∫–∞ —Å—Ç–µ—Ä–µ–æ)
    """
    logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ–º T-Bank VoiceKit —Å –∞–Ω–∞–ª–∏–∑–æ–º –≥–æ–ª–æ—Å–æ–≤ (—Å—Ç–µ—Ä–µ–æ)...")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ —Ñ–∞–π–ª
    try:
        audio = AudioSegment.from_file(audio_file)
        # –ù–ï –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –º–æ–Ω–æ - —Ä–∞–±–æ—Ç–∞–µ–º —Å–æ —Å—Ç–µ—Ä–µ–æ
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ –∞—É–¥–∏–æ: {audio.channels} –∫–∞–Ω–∞–ª–æ–≤, {audio.frame_rate}Hz")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∞—É–¥–∏–æ: {e}")
        return {"success": False, "error": str(e)}
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ —Å–ª–æ–≤
    words_with_time = []
    
    if "results" in transcript_data:
        for result in transcript_data["results"]:
            if "alternatives" in result:
                for alternative in result["alternatives"]:
                    if "words" in alternative:
                        for word in alternative["words"]:
                            words_with_time.append({
                                "word": word["word"],
                                "start_time": float(word["start_time"].rstrip('s')),
                                "end_time": float(word["end_time"].rstrip('s')),
                                "confidence": word.get("confidence", 0.0)
                            })
    
    if not words_with_time:
        logger.warning("–ù–µ –Ω–∞–π–¥–µ–Ω—ã –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ —Å–ª–æ–≤")
        return {"success": False, "error": "–ù–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫"}
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Å–ª–æ–≤–∞ –≤ —Å–µ–≥–º–µ–Ω—Ç—ã –ø–æ –ø–∞—É–∑–∞–º
    segments = []
    current_segment = []
    min_pause = 0.5  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø–∞—É–∑–∞ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —Å–µ–≥–º–µ–Ω—Ç–æ–≤
    
    for i, word in enumerate(words_with_time):
        current_segment.append(word)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—É–∑—É –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–ª–æ–≤–∞
        if i < len(words_with_time) - 1:
            next_word = words_with_time[i + 1]
            pause = next_word["start_time"] - word["end_time"]
            
            if pause > min_pause:
                # –°–æ–∑–¥–∞–µ–º —Å–µ–≥–º–µ–Ω—Ç
                if current_segment:
                    segments.append(current_segment)
                    current_segment = []
    
    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç
    if current_segment:
        segments.append(current_segment)
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≥–æ–ª–æ—Å–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
    known_speakers = []
    speakers_data = []
    
    for i, segment in enumerate(segments):
        if not segment:
            continue
        
        start_time = segment[0]["start_time"]
        end_time = segment[-1]["end_time"]
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –≥–æ–ª–æ—Å–∞
        voice_features = extract_voice_features(audio, start_time, end_time)
        
        # –ò–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä—É–µ–º —Å–ø–∏–∫–µ—Ä–∞ –ø–æ —Å—Ç–µ—Ä–µ–æ –ø–æ–∑–∏—Ü–∏–∏
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—É—é –ª–æ–≥–∏–∫—É: —Ç–æ–ª—å–∫–æ 2 —Å–ø–∏–∫–µ—Ä–∞
        stereo_balance = voice_features.get("stereo_balance", 0)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–ø–∏–∫–µ—Ä–∞ –ø–æ —Å—Ç–µ—Ä–µ–æ –±–∞–ª–∞–Ω—Å—É - —Ç–æ–ª—å–∫–æ 2 —Å–ø–∏–∫–µ—Ä–∞
        if stereo_balance > 0.1:  # –õ–µ–≤—ã–π –∫–∞–Ω–∞–ª –∏–ª–∏ —Ü–µ–Ω—Ç—Ä —Å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º –±–∞–ª–∞–Ω—Å–æ–º
            speaker_id = "SPEAKER_02"
        else:  # –ü—Ä–∞–≤—ã–π –∫–∞–Ω–∞–ª –∏–ª–∏ —Ü–µ–Ω—Ç—Ä —Å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º –±–∞–ª–∞–Ω—Å–æ–º
            speaker_id = "SPEAKER_01"
        
        confidence = 0.7
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ—Ñ–∏–ª—å —Å–ø–∏–∫–µ—Ä–∞
        speaker_profile = None
        for speaker in known_speakers:
            if speaker["speaker_id"] == speaker_id:
                speaker_profile = speaker
                break
        
        if speaker_profile is None:
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞
            speaker_profile = {
                "speaker_id": speaker_id,
                "voice_features": []
            }
            known_speakers.append(speaker_profile)
        
        speaker_profile["voice_features"].append(voice_features)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç —Å–µ–≥–º–µ–Ω—Ç–∞
        text = " ".join([word["word"] for word in segment])
        
        speakers_data.append({
            "speaker": speaker_id,
            "start_time": start_time,
            "end_time": end_time,
            "text": text,
            "confidence": confidence,
            "voice_features": voice_features
        })
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ—Ñ–∏–ª–∏ —Å–ø–∏–∫–µ—Ä–æ–≤
    for speaker in known_speakers:
        speaker.update(analyze_speaker_voice_profile(speaker["voice_features"]))
    
    return {
        "success": True,
        "speakers_data": speakers_data,
        "known_speakers": known_speakers,
        "method": "tbank_with_voice_analysis"
    }

def recognize_with_tbank_voice_diarization(audio_file: str) -> bool:
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Å –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–µ–π —á–µ—Ä–µ–∑ T-Bank VoiceKit
    """
    try:
        print(f"=== T-BANK VOICEKIT –° –ê–ù–ê–õ–ò–ó–û–ú –ì–û–õ–û–°–û–í (–°–¢–ï–†–ï–û) ===")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∞–π–ª
        if not os.path.exists(audio_file):
            print(f"–§–∞–π–ª {audio_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return False
        
        file_size = os.path.getsize(audio_file)
        print(f"–§–∞–π–ª {audio_file} –Ω–∞–π–¥–µ–Ω")
        print(f"–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_size} –±–∞–π—Ç")
        
        # –°–æ–∑–¥–∞–µ–º –∫–ª–∏–µ–Ω—Ç T-Bank
        print(f"\n–°–æ–∑–¥–∞–µ–º –∫–ª–∏–µ–Ω—Ç T-Bank VoiceKit...")
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –∫–ª—é—á–∏
        api_key = os.getenv("TBANK_API_KEY", "LEc1tAfU1qDrn6chWuo/Lau2pJCyHyC/e6FtjquWidM=")
        secret_key = os.getenv("TBANK_SECRET_KEY", "YLWjm7DGJZSZzuJcoaNZTFWDADKtMfuOdrU4rsCRQmU=")
        
        client = ClientSTT(api_key=api_key, secret_key=secret_key)
        print("–ö–ª–∏–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∞—É–¥–∏–æ
        print(f"\n–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∞—É–¥–∏–æ...")
        audio = AudioSegment.from_file(audio_file)
        duration = len(audio) / 1000.0
        sample_rate = audio.frame_rate
        channels = audio.channels
        
        print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞—É–¥–∏–æ: {duration:.2f}—Å, {sample_rate}Hz, {channels} –∫–∞–Ω–∞–ª–æ–≤")
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –Ω—É–∂–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è T-Bank (–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞)
        audio_for_tbank = audio
        
        if channels > 1:
            audio_for_tbank = audio_for_tbank.set_channels(1)
            print("–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –≤ –º–æ–Ω–æ –¥–ª—è T-Bank")
        
        if sample_rate != 16000:
            audio_for_tbank = audio_for_tbank.set_frame_rate(16000)
            print("–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –≤ 16kHz –¥–ª—è T-Bank")
        
        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è T-Bank
        temp_path = "temp_audio.wav"
        audio_for_tbank.export(temp_path, format="wav", parameters=["-ar", "16000", "-ac", "1"])
        print(f"–ê—É–¥–∏–æ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ –¥–ª—è T-Bank: {temp_path}")
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è T-Bank
        audio_config = {
            "encoding": "LINEAR16",
            "sample_rate_hertz": 16000,
            "num_channels": 1,
            "language_code": "ru-RU",
            "enable_automatic_punctuation": True,
            "enable_denormalization": True,
            "enable_rescoring": True,
            "model": "general"
        }
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ
        print(f"\n–ù–∞—á–∏–Ω–∞–µ–º —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ T-Bank —Å –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–µ–π...")
        print("–≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è...")
        
        with open(temp_path, "rb") as audio_file_obj:
            response = client.recognize(audio_file_obj, audio_config)
        
        print("=== –†–ê–°–ü–û–ó–ù–ê–í–ê–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û! ===")
        print(f"–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {response.get('total_billed_time', 0):.2f} —Å–µ–∫")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç
        full_transcript = ""
        if "results" in response:
            for result in response["results"]:
                if "alternatives" in result:
                    for alternative in result["alternatives"]:
                        full_transcript += alternative["transcript"]
        
        print(f"\n=== –ü–û–õ–ù–´–ô –¢–†–ê–ù–°–ö–†–ò–ü–¢ ===")
        print(full_transcript.strip())
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—é —Å –∞–Ω–∞–ª–∏–∑–æ–º –≥–æ–ª–æ—Å–æ–≤ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ —Å—Ç–µ—Ä–µ–æ –∞—É–¥–∏–æ)
        print(f"\n=== –î–ò–ê–†–ò–ó–ê–¶–ò–Ø –° –ê–ù–ê–õ–ò–ó–û–ú –ì–û–õ–û–°–û–í (–°–¢–ï–†–ï–û) ===")
        
        diarization_result = tbank_with_voice_analysis_diarization(response, audio_file)
        
        if not diarization_result["success"]:
            print(f"–û—à–∏–±–∫–∞ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏: {diarization_result['error']}")
            return False
        
        speakers_data = diarization_result["speakers_data"]
        known_speakers = diarization_result["known_speakers"]
        diarization_method = diarization_result["method"]
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ø–∏–∫–µ—Ä–æ–≤
        speakers_list = list(set([s["speaker"] for s in speakers_data]))
        
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–≤–æ—Ä—è—â–∏—Ö: {len(speakers_list)}")
        print(f"–ì–æ–≤–æ—Ä—è—â–∏–µ: {', '.join(speakers_list)}")
        print(f"–ú–µ—Ç–æ–¥ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏: {diarization_method}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ—Ñ–∏–ª–∏ –≥–æ–ª–æ—Å–æ–≤
        print(f"\n=== –ü–†–û–§–ò–õ–ò –ì–û–õ–û–°–û–í (–°–¢–ï–†–ï–û) ===")
        for speaker in known_speakers:
            print(f"{speaker['speaker_id']}:")
            print(f"  –¢–∏–ø –≥–æ–ª–æ—Å–∞: {speaker['voice_type']}")
            print(f"  –ü–æ–∑–∏—Ü–∏—è –≤ —Å—Ç–µ—Ä–µ–æ: {speaker['stereo_position']}")
            print(f"  –û—Å–Ω–æ–≤–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞: {speaker['avg_fundamental_frequency']:.1f} Hz")
            print(f"  –õ–µ–≤—ã–π –∫–∞–Ω–∞–ª F0: {speaker['avg_left_f0']:.1f} Hz")
            print(f"  –ü—Ä–∞–≤—ã–π –∫–∞–Ω–∞–ª F0: {speaker['avg_right_f0']:.1f} Hz")
            print(f"  –°—Ç–µ—Ä–µ–æ –±–∞–ª–∞–Ω—Å: {speaker['avg_stereo_balance']:.3f}")
            print(f"  –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å: {speaker['stability']:.2f}")
            print(f"  –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å —Å—Ç–µ—Ä–µ–æ: {speaker['stereo_stability']:.2f}")
            print(f"  –û–±—Ä–∞–∑—Ü–æ–≤: {speaker['sample_count']}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–µ–≥–º–µ–Ω—Ç—ã —Å –∞–Ω–∞–ª–∏–∑–æ–º –≥–æ–ª–æ—Å–æ–≤
        for i, speaker in enumerate(speakers_data):
            print(f"\n–°–µ–≥–º–µ–Ω—Ç {i+1}: {speaker['speaker']}")
            print(f"–í—Ä–µ–º—è: {speaker['start_time']:.1f}s - {speaker['end_time']:.1f}s")
            print(f"–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {speaker.get('duration', speaker['end_time'] - speaker['start_time']):.1f}—Å")
            print(f"–¢–µ–∫—Å—Ç: {speaker['text']}")
            print(f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {speaker['confidence']:.2f}")
            print(f"–û—Å–Ω–æ–≤–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞: {speaker['voice_features']['fundamental_frequency']:.1f} Hz")
            print(f"–õ–µ–≤—ã–π –∫–∞–Ω–∞–ª F0: {speaker['voice_features']['left_f0']:.1f} Hz")
            print(f"–ü—Ä–∞–≤—ã–π –∫–∞–Ω–∞–ª F0: {speaker['voice_features']['right_f0']:.1f} Hz")
            print(f"–°—Ç–µ—Ä–µ–æ –±–∞–ª–∞–Ω—Å: {speaker['voice_features']['stereo_balance']:.3f}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        print(f"\n–°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã...")
        
        # TXT —Ñ–∞–π–ª
        output_txt = f"{os.path.splitext(audio_file)[0]}_transcript_with_tbank_stereo_diarization.txt"
        with open(output_txt, "w", encoding="utf-8") as f:
            f.write("=== –¢–†–ê–ù–°–ö–†–ò–ü–¶–ò–Ø –° T-BANK VOICEKIT –ò –ê–ù–ê–õ–ò–ó–û–ú –ì–û–õ–û–°–û–í (–°–¢–ï–†–ï–û) ===\n\n")
            f.write(f"–û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {response.get('total_billed_time', 0):.2f} —Å–µ–∫\n")
            f.write(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–≤–æ—Ä—è—â–∏—Ö: {len(speakers_list)}\n")
            f.write(f"–ì–æ–≤–æ—Ä—è—â–∏–µ: {', '.join(speakers_list)}\n")
            f.write(f"–ú–µ—Ç–æ–¥ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏: {diarization_method}\n")
            f.write(f"–ê—É–¥–∏–æ —Ñ–æ—Ä–º–∞—Ç: {channels} –∫–∞–Ω–∞–ª–æ–≤, {sample_rate}Hz\n\n")
            
            f.write("=== –ü–†–û–§–ò–õ–ò –ì–û–õ–û–°–û–í (–°–¢–ï–†–ï–û) ===\n")
            for speaker in known_speakers:
                f.write(f"{speaker['speaker_id']}:\n")
                f.write(f"  –¢–∏–ø –≥–æ–ª–æ—Å–∞: {speaker['voice_type']}\n")
                f.write(f"  –ü–æ–∑–∏—Ü–∏—è –≤ —Å—Ç–µ—Ä–µ–æ: {speaker['stereo_position']}\n")
                f.write(f"  –û—Å–Ω–æ–≤–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞: {speaker['avg_fundamental_frequency']:.1f} Hz\n")
                f.write(f"  –õ–µ–≤—ã–π –∫–∞–Ω–∞–ª F0: {speaker['avg_left_f0']:.1f} Hz\n")
                f.write(f"  –ü—Ä–∞–≤—ã–π –∫–∞–Ω–∞–ª F0: {speaker['avg_right_f0']:.1f} Hz\n")
                f.write(f"  –°—Ç–µ—Ä–µ–æ –±–∞–ª–∞–Ω—Å: {speaker['avg_stereo_balance']:.3f}\n")
                f.write(f"  –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å: {speaker['stability']:.2f}\n")
                f.write(f"  –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å —Å—Ç–µ—Ä–µ–æ: {speaker['stereo_stability']:.2f}\n")
                f.write(f"  –û–±—Ä–∞–∑—Ü–æ–≤: {speaker['sample_count']}\n\n")
            
            f.write("=== –ü–û–õ–ù–´–ô –¢–†–ê–ù–°–ö–†–ò–ü–¢ ===\n")
            f.write(full_transcript.strip() + "\n\n")
            
            f.write("=== –°–ï–ì–ú–ï–ù–¢–´ –° T-BANK VOICEKIT –ò –ê–ù–ê–õ–ò–ó–û–ú –ì–û–õ–û–°–û–í (–°–¢–ï–†–ï–û) ===\n")
            for i, speaker in enumerate(speakers_data):
                f.write(f"\n--- –°–ï–ì–ú–ï–ù–¢ {i+1}: {speaker['speaker']} ---\n")
                f.write(f"–í—Ä–µ–º—è: {speaker['start_time']:.1f}s - {speaker['end_time']:.1f}s\n")
                f.write(f"–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {speaker.get('duration', speaker['end_time'] - speaker['start_time']):.1f}—Å\n")
                f.write(f"–¢–µ–∫—Å—Ç: {speaker['text']}\n")
                f.write(f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {speaker['confidence']:.2f}\n")
                f.write(f"–û—Å–Ω–æ–≤–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞: {speaker['voice_features']['fundamental_frequency']:.1f} Hz\n")
                f.write(f"–õ–µ–≤—ã–π –∫–∞–Ω–∞–ª F0: {speaker['voice_features']['left_f0']:.1f} Hz\n")
                f.write(f"–ü—Ä–∞–≤—ã–π –∫–∞–Ω–∞–ª F0: {speaker['voice_features']['right_f0']:.1f} Hz\n")
                f.write(f"–°—Ç–µ—Ä–µ–æ –±–∞–ª–∞–Ω—Å: {speaker['voice_features']['stereo_balance']:.3f}\n")
        
        # JSON —Ñ–∞–π–ª
        output_json = f"{os.path.splitext(audio_file)[0]}_transcript_with_tbank_stereo_diarization.json"
        full_results = {
            "transcript": full_transcript.strip(),
            "speakers": speakers_list,
            "speaker_segments": speakers_data,
            "known_speakers": known_speakers,
        }
        with open(output_json, "w", encoding="utf-8") as f:
            json.dump(full_results, f, ensure_ascii=False, indent=4)
        
        print(f"–§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
        print(f"  - {output_txt}")
        print(f"  - {output_json}")
        
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        if os.path.exists(temp_path):
            os.remove(temp_path)
            print("–í—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —É–¥–∞–ª–µ–Ω")
        
        print("=== –û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê –£–°–ü–ï–®–ù–û! ===")
        print("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª—ã —Å —Å—É—Ñ—Ñ–∏–∫—Å–æ–º '_tbank_stereo_diarization'")
        return True
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ recognize_with_tbank_voice_diarization: {e}")
        print(f"–û–®–ò–ë–ö–ê –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}")
        return False

if __name__ == "__main__":
    print("üöÄ –ó–∞–ø—É—Å–∫ —Å—Ç–µ—Ä–µ–æ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ T-Bank VoiceKit...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–∞
    if not os.path.exists("6.wav"):
        print("–û–®–ò–ë–ö–ê: –§–∞–π–ª 6.wav –Ω–µ –Ω–∞–π–¥–µ–Ω")
        exit(1)
    
    print("–§–∞–π–ª 6.wav –Ω–∞–π–¥–µ–Ω")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É
    success = recognize_with_tbank_voice_diarization("6.wav")
    
    if success:
        print("\nüéâ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
    else:
        print("\nüí• –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —Å –æ—à–∏–±–∫–∞–º–∏!")